{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yan/Code/cnn-classifier/corpus\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/yan/code/cnn-classifier/corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/yan/Code/cnn-classifier/corpus'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "FLAGS = None\n",
    "MAX_LENGTH = 250\n",
    "EMBEDDING_SIZE = 128\n",
    "MAX_LABEL = 47\n",
    "WORDS_FEATURE = 'words' \n",
    "FEATURE_LEN = 'words_len' \n",
    "BATCH_SIZE = 16\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = open('questions_v2.tsv','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234999, 234999)\n",
      "('cancopsbegivenliedetectortest theonlywitnessisthespouse help', '55')\n"
     ]
    }
   ],
   "source": [
    "MIN_LENGTH=10\n",
    "MAX_LENGTH=250\n",
    "def parse_corpus(corpus):    \n",
    "    X_r = []\n",
    "    y_r = []\n",
    "\n",
    "    for parallel in corpus:\n",
    "        question, label = parallel[:-1].split('\\t')\n",
    "        if question.strip()==\"\" or label.strip()==\"\": continue\n",
    "        X_r.append(question)\n",
    "        y_r.append(label)\n",
    "    return X_r, y_r\n",
    "\n",
    "X_r, y_r = parse_corpus(corpus)\n",
    "print(len(X_r),len(y_r))\n",
    "print(X_r[0],y_r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_pa = dict([(i, y) for i, y in enumerate(set(y_r))])\n",
    "pa_idx = dict(zip(idx_pa.values(), idx_pa.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_r, y_r, test_size=0.1, stratify=y_r, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    vocab = learn.preprocessing.VocabularyProcessor(MAX_LENGTH)\n",
    "\n",
    "    for question in open('questions_v2.tsv'):\n",
    "        body, _ = question.split('\\t')\n",
    "        tokens = [i for i in learn.preprocessing.tokenizer([body])][0]\n",
    "        for token in tokens:\n",
    "            vocab.vocabulary_.add(token)\n",
    "\n",
    "    vocab.save('vocab')\n",
    "    \n",
    "def get_vocab():\n",
    "    if not os.path.exists('vocab'):\n",
    "        build_vocab()\n",
    "    return learn.preprocessing.VocabularyProcessor.restore('vocab')   \n",
    "\n",
    "vocab = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_transform_train = vocab.fit_transform(X_train)\n",
    "x_transform_test = vocab.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.array(list(x_transform_train))\n",
    "x_test = np.array(list(x_transform_test))\n",
    "x_train_len = np.array([np.where(i>0)[0].shape[0] for i in x_train])\n",
    "x_test_len = np.array([np.where(i>0)[0].shape[0] for i in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#{k:v for k,v in idx_pa.items()[:5]}\n",
    "y_train_idx = np.array([pa_idx[int(i)] for i in y_train])\n",
    "y_test_idx = np.array([pa_idx[int(i)] for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only questions with more than 10 words are used in training\n",
    "x_train_len_10_plus = np.where(x_train_len>10)[0]\n",
    "x_train = x_train[x_train_len_10_plus]\n",
    "x_train_len = x_train_len[x_train_len_10_plus] \n",
    "y_train_idx = y_train_idx[x_train_len_10_plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((211337, 250), (211337,), 112332)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(vocab.vocabulary_)\n",
    "x_train.shape,y_train_idx.shape, n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(\n",
    "    logits, labels, mode):\n",
    "  \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "  predicted_classes = tf.argmax(logits, 1)\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\n",
    "            'class': predicted_classes,\n",
    "            'prob': tf.nn.softmax(logits)\n",
    "        })\n",
    "\n",
    "  onehot_labels = tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predicted_classes)\n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_model(features, labels, mode):\n",
    "  \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "  # Convert indexes of words into embeddings.\n",
    "  # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "  # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "  # EMBEDDING_SIZE].\n",
    "  word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "  # Split into list of embedding per word, while removing doc length dim.\n",
    "  # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "  #word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "  cell_fw = tf.nn.rnn_cell.GRUCell(num_units=EMBEDDING_SIZE)\n",
    "  cell_bw = tf.nn.rnn_cell.GRUCell(num_units=EMBEDDING_SIZE)\n",
    "\n",
    "  outputs, encoding = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw,\n",
    "        cell_bw,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=features[FEATURE_LEN],\n",
    "        inputs=word_vectors)\n",
    "  '''\n",
    "  cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "  # Create an unrolled Recurrent Neural Networks to length of\n",
    "  # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "  _, encoding = tf.nn.static_rnn(cell, word_list, dtype=tf.float32)  '''\n",
    "\n",
    "  # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "  # neural network of last step) and pass it as features for softmax\n",
    "  # classification over output classes.\n",
    "  cell_fw_state, cell_bw_state = encoding\n",
    "  encoding = tf.concat(axis=1, values=[cell_fw_state, cell_bw_state])\n",
    "  logits = tf.layers.dense(encoding, MAX_LABEL, activation=None)\n",
    "  return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(256)])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = tf.contrib.layers.embed_sequence(x_train[:5], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "cell_fw = tf.nn.rnn_cell.GRUCell(num_units=EMBEDDING_SIZE)\n",
    "cell_bw = tf.nn.rnn_cell.GRUCell(num_units=EMBEDDING_SIZE)\n",
    "outputs, encoding = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw,\n",
    "    cell_bw,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=x_train_len[:5],\n",
    "    inputs=word_vectors)\n",
    "cell_fw_state, cell_bw_state = encoding\n",
    "encoding = tf.concat(axis=1, values=[cell_fw_state, cell_bw_state])\n",
    "encoding.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_model_dir': '/Users/yan/code/cnn-classifier/corpus/log_birnn/', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "model_fn = rnn_model\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={WORDS_FEATURE: x_train, FEATURE_LEN: x_train_len}, \n",
    "  y=y_train_idx,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  num_epochs=5,\n",
    "  shuffle=True)\n",
    "\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=\"/Users/yan/code/cnn-classifier/corpus/log_birnn/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/yan/code/cnn-classifier/corpus/log_birnn/model.ckpt-3000\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /Users/yan/code/cnn-classifier/corpus/log_birnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.28227, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 2.58049\n",
      "INFO:tensorflow:loss = 2.15128, step = 3101 (38.754 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.62181\n",
      "INFO:tensorflow:loss = 2.13417, step = 3201 (38.151 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3300 into /Users/yan/code/cnn-classifier/corpus/log_birnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.28783.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x16b26a350>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={WORDS_FEATURE: x_test, FEATURE_LEN: x_test_len},\n",
    "      y=y_test_idx,\n",
    "      num_epochs=1,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      shuffle=False)\n",
    "\n",
    "predictions = classifier.predict(input_fn=test_input_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/yan/code/cnn-classifier/corpus/log_birnn/model.ckpt-3300\n",
      "Accuracy (sklearn): 0.580894\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-06-03:49:12\n",
      "INFO:tensorflow:Restoring parameters from /Users/yan/code/cnn-classifier/corpus/log_birnn/model.ckpt-3300\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-06-03:50:52\n",
      "INFO:tensorflow:Saving dict for global step 3300: accuracy = 0.580894, global_step = 3300, loss = 1.44655\n",
      "Accuracy (tensorflow): 0.580894\n"
     ]
    }
   ],
   "source": [
    "y_predicted = np.array(list(p['class'] for p in predictions))\n",
    "y_predicted = y_predicted.reshape(np.array(y_test).shape)\n",
    "\n",
    "# Score with sklearn.\n",
    "score = metrics.accuracy_score(y_test_idx, y_predicted)\n",
    "print('Accuracy (sklearn): {0:f}'.format(score))\n",
    "\n",
    "# Score with tensorflow.\n",
    "scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
